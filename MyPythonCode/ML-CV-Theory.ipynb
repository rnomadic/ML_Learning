{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTZtcJ5o99B7"
      },
      "source": [
        "## 1. In conv layer how parameters are calculated?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z0ebuFXp99B9"
      },
      "outputs": [],
      "source": [
        "    #Let say we have below conv layer.\n",
        "    # input layer\n",
        "    input_shape=(None, 1, 28, 28),\n",
        "\n",
        "    # layer conv2d1\n",
        "    conv2d1_num_filters=32,\n",
        "    conv2d1_filter_size=(5, 5),\n",
        "\n",
        "    # layer maxpool1\n",
        "    maxpool1_pool_size=(2, 2),\n",
        "\n",
        "    # layer conv2d2\n",
        "    conv2d2_num_filters=32,\n",
        "    conv2d2_filter_size=(3, 3),\n",
        "\n",
        "    # layer maxpool2\n",
        "    maxpool2_pool_size=(2, 2),\n",
        "    \n",
        "    # Fully Connected Layer\n",
        "    dense_num_units=256,\n",
        "\n",
        "   # output Layer\n",
        "    output_nonlinearity=lasagne.nonlinearities.softmax,\n",
        "    output_num_units=10,\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqKQw0U99B-"
      },
      "source": [
        "### This outputs the above Layer Information:\n",
        "\n",
        "\n",
        "    0  input     1x28x28\n",
        "    1  conv2d1   32x24x24\n",
        "    2  maxpool1  32x12x12\n",
        "    3  conv2d2   32x10x10\n",
        "    4  maxpool2  32x5x5\n",
        "    5  dense     256\n",
        "    6  output    10\n",
        "and outputs the number of learnable parameters as 217,706\n",
        "\n",
        "### Calculation for the above dimensions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FF87D9eC99B_"
      },
      "outputs": [],
      "source": [
        "\"\"\" The full calculations are as below: \"\"\"\n",
        "\n",
        "  #  name                           size                 parameters\n",
        "---  --------  -------------------------    ------------------------\n",
        "  0  input                       1x28x28                           0\n",
        "  1  conv2d1   (28-(5-1))=24 -> 32x24x24    (5*5*1+1)*32   =     832\n",
        "  2  maxpool1                   32x12x12                           0\n",
        "  3  conv2d2   (12-(3-1))=10 -> 32x10x10    (3*3*32+1)*32  =   9'248\n",
        "  4  maxpool2                     32x5x5                           0\n",
        "  5  dense                           256    (32*5*5+1)*256 = 205'056\n",
        "  6  output                           10    (256+1)*10     =   2'570"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM3ap05399B_"
      },
      "source": [
        "#### Design a network to detect two object classes if you know there is going to be only single instance of each object in the image. How the design changes if multiple, unknown number of instances are present? How the design and strategy changes if the number of object classes to be detected is huge ( > 10K)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xdCM9tX99CA"
      },
      "source": [
        "### Architectures\n",
        "\n",
        "1. Difference between Inception v3 and v4. How does Inception Resnet compare with V4.\n",
        "2. Explain main ideas behind ResNet? Why would you try ResNet over other architectures?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOKsrFq299CA"
      },
      "source": [
        "### Samsung image processing interview questions\n",
        "1. How LSTM works? In detail, all the gates and why sigmoid is used?\n",
        "\n",
        "https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9\n",
        "\n",
        "\n",
        "4. Why inception-v3 is used and why not Resnet is used?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnFD7fRu99CA"
      },
      "source": [
        "### Misleneous\n",
        "1. Implement dropout during forward and backward pass?<br>\n",
        "Was not very hard, you just have to consider what’s happening during testing vs training phase. In this question, the interviewer can test your knowledge on dropout, and backprop\n",
        "\n",
        "2. Neural network training loss/testing loss stays constant, what do you do?<br>\n",
        "Open question (ask if there could be an error in your code, going deeper, going simpler…)\n",
        "\n",
        "3. Why do RNNs have a tendency to suffer from exploding/vanishing gradient?<br>\n",
        "And probably you know the next question… How to prevent this? You can talk about LSTM cell which helps the gradient from vanishing, but make sure you know why it does so. I also remember having a nice conversation about gradient clipping, where we wonder whether we should clip the gradient element wise, or clip the norm of the gradient.\n",
        "\n",
        "4. Do you know GAN, VAE(variational auto encoder), and memory augmented neural network? Can you talk about it? \n",
        "5. Does using full batch means that the convergence is always better given unlimited power?\n",
        "6. What is the problem with sigmoid during backpropagation? <br>\n",
        "Very small, between 0.25 and zero.[2]\n",
        "\n",
        "7. Given a black box machine learning algorithm that you can’t modify, how could you improve its error? <br>\n",
        "Open question, you can transform the input for example.\n",
        "\n",
        "8. How to find the best hyper parameters? <br>\n",
        "Random search, grid search, Bayesian search (and what it is?)\n",
        "\n",
        "9. What is transfer learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytYiYY-99CH"
      },
      "source": [
        "### Q1. Why are CNNs used more for computer vision tasks than other tasks?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDV2qIsm99CI"
      },
      "source": [
        "How our brain does vision. We start simple and get complex. For example, we start with as simple as identifying edge, colors and then build upon them to detect object and then classify them.\n",
        "\n",
        "Architecture of CNNs are designed as such to emulate human brain’s technique to deal with images. As convolutions are mainly used for extracting high level features from the images such as edges/other patterns, these algorithm try to emulate our understanding of the vision.\n",
        "\n",
        "There are certain filters that do operations such as blurring the image, sharpening the image and then performing pooling operations on each of these filters to extract informations from an image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0GoAkZ99CJ"
      },
      "source": [
        "### Q2. What is the best way to do multiple object recognition/classification in real time?Describe the transition between R-CNN, Fast R-CNN and Faster RCNN for object detection.\n",
        "\n",
        "[Deep Learning for Object Detection](https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9)\n",
        "\n",
        "[Stanford_cs231n](http://cs231n.stanford.edu/slides/2018/cs231n_2018_ds06.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoYFJfYz99CJ"
      },
      "source": [
        "One of the fundamental problem with such type of problem is that you can’t apply the fundamental CNN to figure out objects within these. Because the **traditional CNN tend to get confused when there are multiple labels associated with an image**. The reason being that CNN can’t really figure out what is the unique element that differentiates a particular object within an image. For example, in the given image above there is a banana and orange. However, traditional CNN can’t really understand what exactly is banana or an orange in an image. CNN may end up learning say if there is a bag in an image, than it is orange and banana. Because say most image that has label banana and orange tend to have a grocery bag in it or particular colour in it.\n",
        "\n",
        "What can we do?\n",
        "\n",
        "\n",
        "Solution: **R-CNN (Region Based CNN)**\n",
        "\n",
        "\n",
        "Well use CNN with some tweaked algorithm. Traditional approach to multiple object classification in an image would be use object detection algorithm and run CNN on top of that. So you can use sliding window, or another any state of the art object detection algorithm and figure out where the object of interest are. For example figure out a bounding box (that green box) on each images. These bounding box would contain the object of interest within the image such as, apple, cat, oranges, banana etc. And once you have these bounding box figured out you can run traditional CNN on top of the object detector and pass these bonding boxes to your network to do object classification. And eventually put combine all the separated part of an image to form the entire image to provide the result back.\n",
        "\n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/R-CNN.webp?raw=1\">\n",
        "\n",
        "\n",
        "\n",
        "So what is the problem with RCNN?\n",
        "\n",
        "\n",
        "1. Algorithms such as sliding window with certain stride is extremely slow to run on huge images. Even algorithms such as selective search, and faster variant of object detection is pretty slow. RCNN generally proposes about 2k regions and on each of these region CNN will run and extract high level features to do classification. And these region proposal tend to be extremely slow.\n",
        "2. In fact, even when you have the region proposed it takes as slow as 5 seconds on each image to do classification and put it back together.\n",
        "3. You need multi-stage training to run this model. You need to train your object detector algorithm, CNN to extract high level features and one more algorithm to do classification.\n",
        "\n",
        "\n",
        "So how do we overcome this problem?\n",
        "\n",
        "Solution: **Fast-RCNN**\n",
        "\n",
        "Just put CNN at the beginning and object proposal the end. And then SVM or other algorithms to do classification. So first extract the high level feature, run object detection on top of that to propose regions and then do classification. These things are known as Fast-RCNN and were proposed by Microsoft.\n",
        "\n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/Fast-RCNN.webp?raw=1\">\n",
        "\n",
        "Advantage of Fast-RCNN\n",
        "\n",
        "Faster training time. Speed up of 8.8x over RCNN to train.\n",
        "One image takes about 0.32 sec to classify (with regions proposed)\n",
        "So what is the problem with Fast-RCNN?\n",
        "\n",
        "It still takes about 2 second to do the region proposal which is a bottle neck at the test time. And these is not realistic for practical usage.\n",
        "\n",
        "\n",
        "Solution: **RPN (Region Proposal Network)  or Faster-RCNN**\n",
        "\n",
        "Well the problem in both of the above algorithm is object detection. These algorithms are very slow. Is there a way to speed up these algorithm?\n",
        "\n",
        "Easy solution to this is to train RPN (Region Proposal Network), a type of convoluted neural network to do object detection and eventually pass it to the CNN to do the classification. There will be two CNNs in the architecture, one purely to do object detection and other purely to do classification. These is known as Faster-RCNN.\n",
        "\n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/Faster-RCNN.webp?raw=1\">\n",
        "\n",
        "\n",
        "Advantage of the Faster-RCNN\n",
        "\n",
        "Speed up of 250x at train and test time. Takes about 0.2 seconds to do the classification of an image.\n",
        "Realistic time for the practical usage.\n",
        "\n",
        "[Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CksXfsCq99CK"
      },
      "source": [
        "### Q3. What is the difference between Deconvolution, Upsampling, Unpooling, and Convolutional Sparse Coding?\n",
        "\n",
        "**Unpooling** is commonly used in the context of convolutional neural networks to denote reverse max pooling. Citing from this paper: Unpooling: In the convnet, the max pooling operation is non-invertible, however we can obtain an approximate inverse by recording the locations of the maxima within each pooling region in a set of switch variables. In the deconvnet, the unpooling operation uses these switches to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus.\n",
        "\n",
        "**Deconvolution** in the context of convolutional neural networks is often used to denote a sort of reverse convolution, which importantly and confusingly is not actually a proper mathematical deconvolution. In contrast to unpooling, using ‘deconvolution’ the upsampling of an image can be learned. It is often used for upsampling the output of a convnet to the original image resolution. I wrote another answer on this topic here. Deconvolution is more appropriately also referred to as convolution with fractional strides, or transpose convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "RbNxmp_F99CK"
      },
      "source": [
        "### Q4. What is the difference between object detection, semantic segmentation and localization?\n",
        "[General Idea](https://cs.stackexchange.com/questions/51387/what-is-the-difference-between-object-detection-semantic-segmentation-and-local)\n",
        "\n",
        "[Stanford Lecture](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)\n",
        "\n",
        "[Example](https://medium.com/@keremturgutlu/semantic-segmentation-u-net-part-1-d8d6f6005066)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51eTa3c99CK"
      },
      "source": [
        "### Q5. Describe the approach in SSD and YOLO for object detection. Why MobileNet SSD is better in video streams ?\n",
        "[MEDIUM ARTICLE ON YOLO](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
        "\n",
        "[Why ResNet is better](https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624)\n",
        "\n",
        "** Mobilenet - SSD**\n",
        "-----------------------------\n",
        "There are two types of deep neural networks here. **Base network** and **detection network**. AlexNet, MobileNet, VGG-Net, Inception(V3, V4), ResNet(50, 152) and all of them are based on DL networks. The Base network provides high-level features for classification or detection. If you use a fully connected layer at the end of these networks, you have a classification. But you can remove the fully connected layer and replace it with detection networks, like SSD, Faster R-CNN, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Why we need fully connected layer\n",
        "We can divide the whole network (for classification) into two parts:\n",
        "\n",
        "Feature extraction: In the conventional classification algorithms, like SVMs, we used to extract features from the data to make the classification work. The convolutional layers are serving the same purpose of feature extraction. CNNs capture better representation of data and hence we don’t need to do feature engineering.\n",
        "\n",
        "Classification: After feature extraction we need to classify the data into various classes, this can be done using a fully connected (FC) neural network. In place of fully connected layers, we can also use a conventional classifier like SVM. But we generally end up adding FC layers to make the model end-to-end trainable.\n",
        "\n",
        "The output from the convolutional layers represents high-level features in the data. While that output could be flattened and connected to the output layer, adding a fully-connected layer is a (usually) cheap way of learning non-linear combinations of these features.\n",
        "\n",
        "Essentially the convolutional layers are providing a meaningful, low-dimensional, and somewhat invariant feature space, and the fully-connected layer is learning a (possibly non-linear) function in that space.\n",
        "\n",
        "https://towardsdatascience.com/convolutional-neural-network-17fb77e76c05\n"
      ],
      "metadata": {
        "id": "0UMFWIPym8St"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhW5PvFQ99CL"
      },
      "source": [
        "### Q6. Explain semantic segmentation (using UNET)\n",
        "\n",
        "https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47#:~:text=8.-,UNET%20Architecture%20and%20Training,by%20Olaf%20Ronneberger%20et%20al.&text=The%20architecture%20contains%20two%20paths,convolutional%20and%20max%20pooling%20layers.\n",
        "\n",
        "The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented.\n",
        "\n",
        "The expected output in **semantic segmentation** are not just labels and bounding box parameters. The output itself is a high resolution image (typically of the same size as input image) in which each pixel is classified to a particular class. Thus it is a pixel level image classification.\n",
        "\n",
        "**Instance segmentation** is one step ahead of semantic segmentation wherein along with pixel level classification, we expect the computer to classify each instance of a class separately. For example in the image above there are 3 people, technically 3 instances of the class “Person”. All the 3 are classified separately (in a different color). But semantic segmentation does not differentiate between the instances of a particular class.\n",
        "\n",
        "** Application of Semantic Segmentation:**\n",
        "-------------------------------------------------------------\n",
        "\n",
        "**Autonomous driving** is a complex robotics tasks that requires perception, planning and execution within constantly evolving environments. Semantic Segmentation provides information about free space on the roads, as well as to detect lane markings and traffic signs.\n",
        "\n",
        "**Bio Medical Image Diagnosis** - Machines can augment analysis performed by radiologists, greatly reducing the time required to run diagnostic tests.\n",
        "\n",
        "**Geo Sensing** -  To recognize the type of land cover (e.g., areas of urban, agriculture, water, etc.) for each pixel on a satellite image, land cover classification can be regarded as a multi-class semantic segmentation task. Road and building detection is also an important research topic for traffic management, city planning, and road monitoring.\n",
        "TGS is one of the leading Geo-science and Data companies which uses seismic images and 3D renderings to understand which areas beneath the Earth’s surface which contain large amounts of oil and gas.\n",
        "\n",
        "**Precision Agriculture** - Precision farming robots can reduce the amount of herbicides that need to be sprayed out in the fields and semantic segmentation of crops and weeds assist them in real time to trigger weeding actions. Such advanced image vision techniques for agriculture can reduce manual monitoring of agriculture.\n",
        "\n",
        "\n",
        "**Architecture of UNET:**\n",
        "------------------------------------\n",
        "It has 2 part- 1> Contraction path (Encoder) and 2> Expansion Path (Decoder)\n",
        "\n",
        "On a high level, we have the following relationship:\n",
        "Input (128x128x1) => Encoder =>(8x8x256) => Decoder =>Ouput (128x128x1)\n",
        "\n",
        "To get better precise locations, at every step of the decoder we use skip connections by concatenating the output of the transposed convolution layers with the feature maps from the Encoder at the same level:\n",
        "\n",
        "u6 = u6 + c4 <br>\n",
        "u7 = u7 + c3 <br>\n",
        "u8 = u8 + c2 <br>\n",
        "u9 = u9 + c1 <br>\n",
        "\n",
        "After every concatenation we again apply two consecutive regular convolutions so that the model can learn to assemble a more precise output\n",
        "\n",
        "<img src='https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/UNET.jpeg?raw=1'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgidRHWf99CL"
      },
      "source": [
        "### Q7 Difference between various Deep Network Architecture (AlexNet, VGGNet, ResNet, and Inception)\n",
        "\n",
        "https://towardsdatascience.com/an-intuitive-guide-to-deep-network-architectures-65fdc477db41\n",
        "\n",
        "https://towardsdatascience.com/the-w3h-of-alexnet-vggnet-resnet-and-inception-7baaaecccc96\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oiuk2Cnm99CL"
      },
      "source": [
        "### Q8. Explain CRAFT model that is used for identifying region of interest in a scene\n",
        "\n",
        "https://towardsdatascience.com/pytorch-scene-text-detection-and-recognition-by-craft-and-a-four-stage-network-ec814d39db05\n",
        "\n",
        "CRAFT adopts a fully convolutional network architecture based on **VGG-16 or ResNet** as its backbone. VGG16/ResNet is essentially the feature extracting architecture that is used to **encode the network’s input into a certain feature representation**. The **decoding segment** of the CRAFT network is similar to **UNet**. It has skip connections that aggregate low-level features.\n",
        "\n",
        "CRAFT predicts two scores for each character:\n",
        "\n",
        "**Region Score:** As the name suggests, it gives the region of the character. It localizes the character.\n",
        "\n",
        "**Affinity Score:** ‘Affinity’ is the degree to which a substance tends to combine with another. So, an affinity score merges characters into a single instance (a word).\n",
        "\n",
        "CRAFT generates two maps as output: **Region Level Map** and **Affinity Map**.\n",
        "\n",
        "    1>  The areas where the characters are present are marked in the Region Map:\n",
        "    2> The Affinity Map is a pictorially represents the related character. Red symbolizes the characters have a high affinity and must be merged into a word.\n",
        "    3> Finally, the affinity and region scores are combined to give the bounding box of each word. The coordinates are in the order: (left-top), (right-top) (right-bottom), (left-bottom), where each coordinate is an (x, y) pair.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RADztIOS99CL"
      },
      "source": [
        "### Q8. VGG 16 Architecture\n",
        "\n",
        "https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c#:~:text=VGG16%20is%20a%20convolution%20neural,vision%20model%20architecture%20till%20date.&text=It%20follows%20this%20arrangement%20of,consistently%20throughout%20the%20whole%20architecture.\n",
        "\n",
        "\n",
        "\n",
        "<img src= 'vgg_16.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36wWpKoc99CM"
      },
      "source": [
        "### 8. What is GAN and how it works?\n",
        "\n",
        "Please check Joe/deep-learning/gan_mnist/Intro_to_GANs_Solution.ipynb for tensorflow solution.\n",
        "\n",
        "A GAN has two parts in it: the generator that generates images and the discriminator that classifies real and fake images.\n",
        "\n",
        "A GAN can be trained to generate images from random noises. For example, we can train a GAN to generate digit images that look like hand-written digit images from MNIST database.\n",
        "\n",
        "\n",
        "#### The Generator\n",
        "The input to the generator is a series of randomly generated numbers called latent sample. Once trained, the generator can produce digit images from latent samples.\n",
        "\n",
        "Our generator is a simple fully connected network that takes a latent sample (100 randomly generated numbers) and produces 784 data points which can be reshaped into a 28 x 28 digit image which is the size used by all MNIST digit images.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52vXQljd99CM"
      },
      "outputs": [],
      "source": [
        "generator = Sequential([\n",
        "    Dense(128, input_shape=(100,)),\n",
        "    LeakyReLU(alpha=0.01),\n",
        "    Dense(784),\n",
        "    Activation('tanh')\n",
        "], name='generator')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFhoPABc99CM"
      },
      "source": [
        "#### The Discriminator\n",
        "The discriminator is a classifier trained using the supervised learning. It classifies whether an image is real (1) or not (0).\n",
        "The discriminator is also a simple fully connected neural network. The last activation is sigmoid to tell us the probability of whether the input image is real or not. So, the output can be any value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWJEmmJE99CM"
      },
      "outputs": [],
      "source": [
        "discriminator = Sequential([\n",
        "    Dense(128, input_shape=(784,)),\n",
        "    LeakyReLU(alpha=0.01),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid')\n",
        "], name='discriminator')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ukcwr3Z99CN"
      },
      "source": [
        "### 9. What is the difference between GAN and DCGAN? Explain the DCGAN project on SVHN data set. How many classes are there in SVHN\n",
        "A Deep Convolution GAN (DCGAN) does something very similar to GAN, but specifically focusses on using Deep Convolutional networks in place of those fully-connected networks. Conv nets in general find areas of correlation within an image, that is, they look for spatial correlations. This means a DCGAN would likely be more fitting for image/video data.\n",
        "\n",
        "#### The Difference between the Simple GAN and the DCGAN\n",
        "Here is the summary of DCGAN:\n",
        "\n",
        "- Replace all max pooling with convolutional stride\n",
        "- Use transposed convolution for upsampling.\n",
        "- Eliminate fully connected layers.\n",
        "- Use Batch normalization except the output layer for the generator and the input layer of the discriminator.\n",
        "- Use ReLU in the generator except for the output which uses tanh.\n",
        "- Use LeakyReLU in the discriminator.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cUWvydy99CN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ML-CV-Theory.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}