{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnomadic/ML_Learning/blob/main/MyPythonCode/Probability_Statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9DUDOjZI7ss"
      },
      "source": [
        "### 1. If I draw three cards at random (without replacement) from a standard 52-card deck, what is the probability that two of the cards will be black and one of them will be red?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOMD3rOXI7su"
      },
      "source": [
        "When we draw the first card, there are 26 black cards and 52 cards total, so there is a 26/52=1/2 chance that this card is black. \n",
        "\n",
        "If the first card is black, there are 25 black cards out of 51 total when we draw the second card, so there is a 25/51 chance it is black. \n",
        "\n",
        "If the first two cards were both black, then there are 26 red cards out of 50 total when we draw the third card, so there is a 26/50=13/25 chance it is red. \n",
        "\n",
        "Thus P(BBR)=(1/2)*(25/51)*(13/25)=13/102."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAQYRNG_I7sv"
      },
      "source": [
        "### 2. 1% of people have a certain genetic defect. 90% of tests for the gene detect the defect (true positives). 9.6% of the tests are false positives. If a person gets a positive test result, what are the odds they actually have the genetic defect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTb9DD-KI7sv"
      },
      "source": [
        "$ P(A/X) = P(X/A).P(A) / P(X/A).P(A) + P(X/ not A) . P(not A)$\n",
        "\n",
        "A = chance of having the faulty gene. That was given in the question as 1%. That also means the probability of not having the gene (~A) is 99%. \n",
        "\n",
        "X = A positive test result.\n",
        "\n",
        "P(A|X) = Probability of having the gene given a positive test result.\n",
        "\n",
        "P(X|A) = Chance of a positive test result given that the person actually has the gene. That was given in the question as 90%.\n",
        "\n",
        "p(X|~A) = Chance of a positive test if the person doesn’t have the gene. That was given in the question as 9.6%\n",
        "\n",
        "Now we have all of the information we need to put into the equation:\n",
        "P(A|X) = (.9 * .01) / (.9 * .01 + .096 * .99) = 0.0865 (8.65%).\n",
        "\n",
        "The probability of having the faulty gene on the test is 8.65%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy9qJNdvI7s2"
      },
      "source": [
        "### 3. Given the following statistics, what is the probability that a woman has cancer if she has a positive mammogram result?\n",
        "\n",
        "One percent of women over 50 have breast cancer.\n",
        "\n",
        "Ninety percent of women who have breast cancer test positive on mammograms.\n",
        "\n",
        "Eight percent of women will have false positives.\n",
        "\n",
        "Having cancer is A and a positive test result is X.\n",
        "\n",
        "P(A)=0.01\n",
        "\n",
        "P(~A)=0.99\n",
        "\n",
        "P(X|A)=0.9\n",
        "\n",
        "P(X|~A)=0.08\n",
        "\n",
        "Insert the parts into the equation and solve. Note that as this is a medical test, we’re using the form of the equation from example \n",
        "\n",
        "(0.9 * 0.01) / ((0.9 * 0.01) + (0.08 * 0.99) = 0.10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5rHm0iI7s3"
      },
      "source": [
        "## Q1 Linear Regression\n",
        "### Q1.1 How do you measeure the performance of Linear Regression?\n",
        "\n",
        "There are 3 main metrics for model evaluation in regression:\n",
        "1. R Square/Adjusted R Square (Describe in detail in Q1.3)\n",
        "2. Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
        "3. Mean Absolute Error(MAE)\n",
        "\n",
        "    R Square measures how much of variability in dependent variable can be explained by the model. It is square of Correlation Coefficient(R) and that is why it is called R Square.\n",
        "    \n",
        "    While R Square is a relative measure of how well the model fits dependent variables, Mean Square Error is an absolute measure of the goodness for the fit.\n",
        "    \n",
        "    MSE is calculated by the sum of square of prediction error which is real output minus predicted output and then divide by the number of data points. It gives you an absolute number on how much your predicted results deviate from the actual number. You cannot interpret much insights from one single result but it gives you an real number to compare against other model results and help you select the best regression model.\n",
        "    \n",
        "    Root Mean Square Error(RMSE) is the square root of MSE. It is used more commonly than MSE because firstly sometimes MSE value can be too big to compare easily. Secondly, MSE is calculated by the square of error, and thus square root brings it back to the same level of prediction error and make it easier for interpretation\n",
        "    \n",
        "    <img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MSE_RMSE.png?raw=1\" width=\"200\">\n",
        "    \n",
        "   **MSE** gives larger penalisation to big prediction error by square it while **MAE treats all errors the same**.\n",
        "   \n",
        "     <img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MAE.png?raw=1\" width=\"200\">\n",
        "    \n",
        "    R Square/Adjusted R Square are better used to explain the model to other people because you can explain the number as a percentage of the output variability. MSE, RMSE or MAE are better to be used to compare performance between different regression models.\n",
        "\n",
        "### Q1.2 How to minimize the MSE\n",
        "There are 2 methods available. <br>\n",
        "1> Gradient Descent\n",
        "2> Ordinary Least Square (OLS)\n",
        "\n",
        "[Difference Between OLS and GD](https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76)\n",
        "\n",
        "### Q1.3 How do you detect goodness of fit? What is R-squared and what does it do?\n",
        "MSE is not perfect. **R-squared is used to assess the goodness of fit** of our regression model. <br>\n",
        "\"How much of my change in the O/P (y) is explained by the change in my input(x)\"<br>\n",
        "0.0 < r^2 < 1.0 <br>\n",
        "0.0 -> Line is not doing good in captureing trend in data <br>\n",
        "1.0 -> Line does good job of desribing relationship between I/P(x) and O/P(y). <br>\n",
        "\n",
        "If R-squared=0.93 then it means 93% variations in dependent variable Y are explained by the independent variables present in our model. <br>\n",
        "\n",
        "Higher the value of R Squared examined as higher the coherence and predictive ability of the model. <br>\n",
        "\n",
        "In R-squared we have a baseline model which is the worst model. This baseline model doesn’t make use of any independent variables to predict the value of dependent variable Y. Instead it uses the mean of the observed responses of dependent variable Y and always predicts this mean as the value of Y.\n",
        "\n",
        "R Squared = 1- (SSR/SST) where, <br>\n",
        "SSR = Sum of Squared Residuals <br>\n",
        "SST = Sum of Squared Total <br>\n",
        "\n",
        "#Perform linear regression using sklearn library <br>\n",
        "regressor = LinearRegression() <br>\n",
        "regressor.fit(X_train,y_train)   <br>      \n",
        "predictions = regressor.predict(X_test) <br>\n",
        "#sklearn's inbuilt method for computing the RSquared of the model <br>\n",
        "rsquared = regressor.score(X_test, y_test) <br>\n",
        "\n",
        "#Adjusted RSquared of the model <br>\n",
        "n=len(data) #number of records <br>\n",
        "p=len(data.columns)-2 #number of features .i.e. columns excluding uniqueId and target variable <br>\n",
        "adjr= 1-(1-score)*(n-1)/(n-p-1) <br>\n",
        "\n",
        "\n",
        "Oops !!! It is less than R squared. Moreover, there is a drop of around 2% in the confidence from R Squared (0.963) to Adjusted R Squared (0.945).\n",
        "\n",
        "\n",
        "### Q1.4 What is the difference between R sqare and adjusted R square? Is adjusted R square value always lesser than R sqaure value? \n",
        "\n",
        "https://towardsdatascience.com/the-enigma-of-adjusted-r-squared-57b01edac9f\n",
        "\n",
        "**What is the limitation of R squared**\n",
        "R Squared = 1- (SSR/SST) <br>\n",
        "SST = how much do the predicted points get varies from the mean of the target variable <br>\n",
        "SST = Sum (Square (Each data point — Mean of the target variable)) <br>\n",
        "SSR = Sum (Square (Each data point — Each corresponding data point in the regression line))<br>\n",
        "\n",
        "Let say we want to build a regression model to predict the height of a person with weight as the independent variable <br>\n",
        "\n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/regression.png?raw=1\" width=\"500\">\n",
        "\n",
        "In the above diagram, let’s consider that the blue line indicates the predictions <br>\n",
        "The red line in the following diagram shows the mean value of the height of all the persons belongs to our sample <br>\n",
        "\n",
        "Here,<br>\n",
        "SST will be a large number because it is a very poor model (red line). <br>\n",
        "SSR will be a small number because it is the best model. <br>\n",
        "So, SSR/SST will be a very small number (It will become very small whenever SSR decreases). <br>\n",
        "So, 1- (SSR/SST) will be a large number. <br>\n",
        "So we can infer that whenever R Squared goes higher, it means the model is too good. <br>\n",
        "\n",
        "But in the real case, we will have 100’s of independent variables for a single dependent variable. The actual problem is that, out of 100’s of independent variables-\n",
        "\n",
        "    Some variables will have a very high correlation with the target variable.\n",
        "    Some variables will have a very small correlation with the target variable.\n",
        "    Also, some independent variables will not correlate at all.\n",
        "    \n",
        "If there is no correlation then what happens is that — **Our model will automatically try to establish a relationship with dependent and independent variables and proceed with mathematical calculations assuming that the researcher has already eliminated the unwanted independent variables.**\n",
        "\n",
        "For example, <br>\n",
        "For predicting the height of a person, we will have the following independent variables <br>\n",
        "Weight ( High correlation ) <br>\n",
        "Phone number( No correlation ) <br>\n",
        "Location ( Low correlation ) <br>\n",
        "Age ( High correlation ) <br>\n",
        "Gender ( Low correlation ) <br>\n",
        "Here, only weight and age are enough to build an accurate model but the model will assume that the phone number will also influence the height and represent it in a multidimensional space. When a regression plane is built through these 5 independent variables, it’s gradient, intercept, cost and residual will automatically adjust to increase the accuracy. When the accuracy gets increases artificially, obviously R squared will also increase.\n",
        "In such scenarios, the regression plane will touch all the edges of the original data points in the multidimensional space. It will make the SSR a very small number and that will eventually make the R Squared as a very high number but when test data is introduced, such models will fail miserably. <br>\n",
        "That is the reason why a high R Squared value does not guarantee an accurate model.\n",
        "\n",
        "\n",
        "\n",
        "**Adjusted R squared value**\n",
        "\n",
        "Adjusted R Squared= 1 — (A * B) <br>\n",
        "where, <br>\n",
        "A = 1 — R Squared <br>\n",
        "B = (n-1) / (n-p-1) <br>\n",
        "p = number of independent variables. <br>\n",
        "n = number of records in the data set. <br>\n",
        "\n",
        "When the number of predictor variables increases, it will decrease the whole value of B. <br>\n",
        "When the value of R Squared increases, it will decrease the whole value of A. <br>\n",
        "Hence technically, it penalizes the value of both A and B if either R Squared is high or the number of predictor variables is high. <br>\n",
        "\n",
        "**In summary, whenever the number of independent variables gets increases, it will penalize the formula so that the total value will come down. It is least affected by the increase of independent variables.** Hence, Adjusted R Squared will more accurately indicate the performance of the model than the R Squared.\n",
        "\n",
        "\n",
        "### Q1.5 What is regularization, why do we use it, and give some examples of common methods?\n",
        "In supervised machine learning, models are trained on a subset of data aka training data. The goal is to compute the target of each training example from the training data.\n",
        "\n",
        "Now, overfitting happens when model learns signal as well as noise in the training data and wouldn’t perform well on new data on which model wasn’t trained on. \n",
        "\n",
        "Now, there are few ways you can avoid overfitting your model on training data like **cross-validation sampling**, **reducing number of features (dimensionality reduction)**, **pruning**, **regularization** etc.\n",
        "\n",
        "Regularization basically adds the penalty as model complexity increases. Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and won’t overfit.\n",
        "\n",
        "    Problem of overfitting:\n",
        "\n",
        "<img src=https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/regularization.png?raw=1 height=300, width=500>\n",
        "\n",
        "     Variance needs to be reduced and as a consequence Bias will be increased: Since Variance is a function of coefficients β1, β2; Bias will also be tagged to them and changed as a function of coefficients β1, β2.\n",
        "     \n",
        "     **Regularization Parameter ‘λ’:** Since both Variance and Bias are functions of coefficients β1, β2, they will be directly proportional. This will not work. Hence we need an additional parameter that can regulate the size of Bias term.\n",
        "     \n",
        "      ‘λ’ is a Hyper-parameter: If ‘λ’ was a parameter, Gradient Descent would nicely set it to 0 and travel to the global minimum. Hence, the control on ‘λ’ cannot be given to Gradient Descent and needs to be kept out. It will not be a parameter but a Hyper-parameter.\n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "### Q1.6 Comparison between OLS, Ridge and Lasso Regression\n",
        "In order to create less complex (parsimonious) model when you have a large number of features in your dataset, some of the Regularization techniques used to address over-fitting and feature selection are:\n",
        "\n",
        "1. L1 Regularization\n",
        "\n",
        "2. L2 Regularization\n",
        "\n",
        "A regression model that uses **L1** regularization technique is called **Lasso Regression** and model which uses **L2** is called **Ridge Regression**. <br>\n",
        "\n",
        "The key difference between these two is the penalty term. <br>\n",
        "\n",
        "Ridge regression adds **“squared magnitude”** of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.\n",
        "\n",
        "Ridge Cost Function:\n",
        "<img src = 'Ridge_Regression.png' Height=300, Width=400>\n",
        "\n",
        "    β1 and β2 can vary in the process of finding the global minimum error. With the regularization term (highlighted in blue in     above equation); a particular value of (β1,β2) will generate a bias output β1² + β2². There can be multiple values that can     generate the same bias like (1,0),(-1,0), (0,-1), (0.7,0.7) and (0,1). In case of L2 Norm, the various combinations of β1       and β2 that generate a particular same Bias, form a circle. For our example, let’s consider a bias term = 1. Fig 6(a)           represents this circle.\n",
        "    \n",
        "    Fig 6(b) indicates the Gradient Descent Contour plot of Linear Regression problem. Now, there are 2 forces at work here.\n",
        "    Force 1: Bias term pulling β1 and β2 to lie somewhere on the black circle only.\n",
        "    Force 2: Gradient Descent trying to travel to the global minimum indicated by green dot.\n",
        "    Both the forces pull and finally settle near the point of intersection indicated by ‘Red cross’.\n",
        "    \n",
        "<img src = 'L2_Norm.png' Height=800, Width=600>\n",
        "\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds **“absolute value of magnitude”** of coefficient as penalty term to the loss function.\n",
        "\n",
        "Lasso Cost Function:\n",
        "<img src = 'Lasso_Regression.png' Height=400, Width=600>\n",
        "\n",
        "Figure below indicates the shape of L1 Norm.\n",
        "<img src = 'L1_Norm.png' Height=400, Width=600>\n",
        "\n",
        "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "\n",
        "L1 vs L2:\n",
        "\n",
        "<img src = 'L2_Vs_L1.png' Height=200, Width=200>\n",
        "\n",
        "\n",
        "### Q1.7. What are the assumptions required for linear regression?\n",
        "\n",
        "#### The four assumptions are:\n",
        "1. Linearity of residuals\n",
        "2. Independence of residuals\n",
        "3. Normal distribution of residuals\n",
        "4. Homoscedasticity (Equal variance of residuals)\n",
        "\n",
        "**Linearity – :**\n",
        "we draw a scatter plot of residuals and y values. Y values are taken on the vertical y axis, and standardized residuals (SPSS calls them ZRESID) are then plotted on the horizontal x axis. If the scatter plot follows a linear pattern (i.e. not a curvilinear pattern) that shows that linearity assumption is met. <br>\n",
        "\n",
        "**Independence -:**\n",
        "We generally have two types of data: cross sectional and longitudinal. Cross -sectional datasets are those where we collect data on entities only once. For example we collect IQ and GPA information from the students at any one given time (think: camera snap shot)\n",
        "\n",
        "Longitudinal data set is one where we collect GPA information from the same student over time (think: video).\n",
        "\n",
        "we worry about Independence when we have longitudinal dataset. In cross sectional datasets we do not need to worry about Independence assumption. It is “assumed” to be met. <br>\n",
        "\n",
        "\n",
        "**Normality -:** we draw a histogram of the residuals, and then examine the normality of the residuals. If the residuals are not skewed, that means that the assumption is satisfied.\n",
        "![title](https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/Normality.png?raw=1)\n",
        "\n",
        "Even though is slightly skewed, but it is not hugely deviated from being a normal distribution. We can say that this distribution satisfies the normality assumption. <br>\n",
        "\n",
        "\n",
        "**Homoscedasticity -:** We look at the scatter plot which we drew for linearity (see above) – i.e. y on the vertical axis, and the ZRESID (standardized residuals) on the x axis. If the residuals do not fan out in a triangular fashion that means that the equal variance assumption is met.\n",
        "![title](https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/Homoscedacity.png?raw=1)\n",
        "\n",
        "In the above picture both linearity and equal variance assumptions are met. It is linear because we do not see any curve in there. It also meets equal variance assumption because we do not see the residuals “dots” fanning out in any triangular fashion.\n",
        "\n",
        "![title](https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/MyPythonCode/resources/Homoscedacity_Violated.png?raw=1)\n",
        "\n",
        "Linearity assumption is violated – there is a curve. Equal variance assumption is also violated, the residuals fan out in a “triangular” fashion.\n",
        "\n",
        "### Q1.8. Regression Loss Types\n",
        "In regression problem we can't use cross entropy. Which gives the class error. Cross entropy value decreases when predicted class is more close to the true class.\n",
        "1. L1 loss\n",
        "2. MSE loss\n",
        "3. smooth L1 loss\n",
        "\n",
        "##### L1 Loss\n",
        "Measure the element wise difference between Predicted and Targeted value. Let say we are trying to predict the centre of an object. Loss will the distance between predicted and actual centre point. Good for large type of error.\n",
        "\n",
        "##### MSE loss\n",
        "It does give huge error for outliers as it is good for small type of error.\n",
        "\n",
        "##### smooth L1 loss\n",
        "It takes the good of both the approach.\n",
        "Uses L1 for large error.\n",
        "Uses MSE for small error\n",
        "\n",
        "\n",
        "### Q1.9. Why ANOVA is Really a Linear Regression, Despite the Difference in Notation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q 1.10 Explain Different types of Crsoss Validation Method\n",
        "**Why -** We often randomly split the dataset into train data and test data to develop a machine learning model.\n",
        "\n",
        "With the change in the random state of the split, the accuracy of the model also changes, so we are not able to achieve a fixed accuracy for the model. \n",
        "\n",
        "**How -** Cross-Validation also referred to as out of sampling technique. It is a resampling procedure used to evaluate machine learning models and access how the model will perform for an independent test dataset.\n",
        "\n",
        "8 different cross-validation techniques\n",
        "\n",
        "    1.Leave p out cross-validation\n",
        "    2.Leave one out cross-validation\n",
        "    3.Holdout cross-validation\n",
        "    4.Repeated random subsampling validation\n",
        "    5.k-fold cross-validation\n",
        "    6.Stratified k-fold cross-validation\n",
        "    7.Time Series cross-validation\n",
        "    8.Nested cross-validation\n",
        "\n",
        "  In **k-fold cross-validation**, the original dataset is equally partitioned into k subparts or folds. Out of the k-folds or groups, for each iteration, one group is selected as validation data, and the remaining (k-1) groups are selected as training data."
      ],
      "metadata": {
        "id": "6iC0YGtW_iq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.11 Why K-Fold cross validation doesn't work with imbalanced dataset\n",
        "\n",
        "A 10-fold cross-validation, in particular, the most commonly used error-estimation method in machine learning, can easily break down in the case of class imbalances, even if the skew is less extreme than the one previously considered. \n",
        "\n",
        "The reason is that the data is split into k-folds with a uniform probability distribution.\n",
        "\n",
        "This might work fine for data with a balanced class distribution, but when the distribution is severely skewed, it is likely that one or more folds will have few or no examples from the minority class. This means that some or perhaps many of the model evaluations will be misleading, as the model need only predict the majority class correctly.\n",
        "\n",
        "**stratified k-fold cross-validation** will enforce the class distribution in each split of the data to match the distribution in the complete training dataset."
      ],
      "metadata": {
        "id": "QHgiKM98_x9-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbCEUjgOI7s7"
      },
      "source": [
        "## Q2 Statistical questions\n",
        "https://www.analyticsvidhya.com/blog/2015/06/correlation-common-questions/\n",
        "\n",
        "### Q2.1 What is covariance and What is corelation? \n",
        "    Covariance signifies the direction of the linear relationship between the two variables. By direction we mean if the variables are directly proportional or inversely proportional to each other. (Increasing the value of one variable might have a positive or a negative impact on the value of the other variable).\n",
        "    \n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/covariance.png?raw=1\" width=\"300\">\n",
        "\n",
        "x_bar and y_bar are the mean.\n",
        "\n",
        "    Correlation shows the kind of relation (in terms of direction) and how strong the relationship is. Thus, we can say the correlation values have standardized notions, whereas the covariance values are not standardized and cannot be used to compare how strong or weak the relationship is because the magnitude has no direct significance. \n",
        "    \n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/correlation.png?raw=1\" width=\"300\">\n",
        "\n",
        "\n",
        "\n",
        "sigma\t=\tpopulation standard deviation\n",
        "N\t=\tthe size of the population\n",
        "x_i\t=\teach value from the population\n",
        "mu\t=\tthe population mean\n",
        "\n",
        "### Q2.2 What is confounding variable\n",
        "In an experiment, the independent variable typically has an effect on your dependent variable. For example, if you are researching whether lack of exercise leads to weight gain, then lack of exercise is your independent variable and weight gain is your dependent variable. Confounding variables are any other variable that also has an effect on your dependent variable. They are like extra independent variables that are having a hidden effect on your dependent variables. Confounding variables can cause two major problems:\n",
        "\n",
        "    Increase variance\n",
        "    Introduce bias.\n",
        "\n",
        "### Q2.3 Can dependent (target) variable have collinearity with independent variable? \n",
        "https://towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea\n",
        "http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/EPIB-621/logconfound.pdf\n",
        "    \n",
        "    Some variables will have a very high correlation with the target variable.\n",
        "    Some variables will have a very small correlation with the target variable.\n",
        "    Also, some independent variables will not correlate at all.\n",
        "    \n",
        "\n",
        "### Q2.4 How you detect multi colinearity?\n",
        "    1. The first simple method is to plot the correlation matrix of all the independent variables.\n",
        "    \n",
        "    #plot color scaled correlation matrix\n",
        "    corr=house_selected.corr()\n",
        "    corr.style.background_gradient(cmap='coolwarm')\n",
        "    \n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/correlation_matrix.png?raw=1\">\n",
        "    \n",
        "From the above figure we can see that there is one pair of independent variables with more than 0.8 correlation which are total basement surface area and first floor surface area. Houses with larger basement area tend to have bigger first floor area as well and so the high correlation should be expected.\n",
        "\n",
        "    2. The second method to check multi-collinearity is to use Variance Inflation Factor(VIF) for each independent variable. It is a measure of multi-collinearity in the set of multiple regression variables. \n",
        "    The higher the value of VIF the higher correlation between this variable and the rest.\n",
        "    \n",
        "    #Compute VIF data for each independent variable\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"features\"] = house_selected.columns\n",
        "    vif[\"vif_Factor\"] = [variance_inflation_factor(house_selected.values, i) for i in range(house_selected.shape[1])]\n",
        "  \n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/VIF.png?raw=1\">\n",
        "\n",
        "**How to fix Multi-Collinearity issue?**\n",
        "    \n",
        "    1. Variable Selection\n",
        "     Remove some variables that are highly correlated and leave the more significant ones in the set.\n",
        "    \n",
        "For example, when we plot the correlation matrix with ‘SalePrice’ included, we can see that Overall Quality and Ground living area have the two highest correlations with dependent variable ‘SalePrice’ and thus I will try to include them in the model.\n",
        "\n",
        "    2. Variable Transformation\n",
        "    \n",
        "The second method is to transform some of the variables to make them less correlated but still maintain their feature. What do I mean by this? In the housing model example, I can transfer ‘years of built’ to ‘age of the house’ by subtracting current year by years of built. \n",
        "\n",
        "### Q2.4 what a p-value is and its limitations in decisions\n",
        "\n",
        "Hypothesis testing is used to test the validity of a claim (null hypothesis) that is made about a population using sample data. The alternative hypothesis is the one you would believe if the null hypothesis is concluded to be untrue. <br>\n",
        "\n",
        "In other words, we’ll make a claim (null hypothesis) and use a sample data to check if the claim is valid. If the claim isn’t valid, then we’ll choose our alternative hypothesis instead.\n",
        "\n",
        "    A small p-value (typically ≤ 0.5) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.\n",
        "\n",
        "    A large p-value (> 0.5) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.\n",
        "\n",
        "    p-values very close to the cutoff (0.5) are considered to be marginal (could go either way).\n",
        "    \n",
        "### Q2.5 How does p-value help in feature selection?\n",
        "https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axxZHgNCI7s8"
      },
      "source": [
        "### Q2.6 When to use different statistical distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBdGDnRpI7s9"
      },
      "source": [
        "**Types of Distributions**\n",
        "\n",
        "    1. Bernoulli Distribution\n",
        "A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial.\n",
        "\n",
        "Here, the occurrence of a head denotes success, and the occurrence of a tail denotes failure.\n",
        "Probability of getting a head = 0.5 = Probability of getting a tail since there are only two possible outcomes.\n",
        "\n",
        "\n",
        "    2. Uniform Distribution\n",
        "When you roll a fair die, the outcomes are 1 to 6. The probabilities of getting these outcomes are equally likely and that is the basis of a uniform distribution. Unlike Bernoulli Distribution, all the n number of possible outcomes of a uniform distribution are equally likely.\n",
        "\n",
        "\n",
        "    3. Binomial Distribution\n",
        "An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial.\n",
        "\n",
        "    4. Normal Distribution\n",
        "Any distribution is known as Normal distribution if it has the following characteristics:\n",
        "\n",
        "a>  The mean, median and mode of the distribution coincide.\n",
        "b>  The curve of the distribution is bell-shaped and symmetrical about the line x=μ.\n",
        "c>  The total area under the curve is 1.\n",
        "d>  Exactly half of the values are to the left of the center and the other half to the right.\n",
        "\n",
        "    5. Poisson Distribution\n",
        "Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. \n",
        "\n",
        "A distribution is called Poisson distribution when the following assumptions are valid:\n",
        "\n",
        "a>  Any successful event should not influence the outcome of another successful event.\n",
        "b>  The probability of success over a short interval must equal the probability of success over a longer interval.\n",
        "c>  The probability of success in an interval approaches zero as the interval becomes smaller.\n",
        "\n",
        "    6. Exponential Distribution\n",
        "    \n",
        "Let’s consider the call center example one more time. What about the interval of time between the calls ? Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls.\n",
        "\n",
        "Other examples are:\n",
        "\n",
        "1. Length of time beteeen metro arrivals,\n",
        "2. Length of time between arrivals at a gas station\n",
        "3. The life of an Air Conditioner\n",
        "\n",
        "Exponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH8DMtiVI7s9"
      },
      "source": [
        "### Q2.7 Bayes Theorem (applied calculations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkJ4rBdhI7s9"
      },
      "source": [
        "https://machinelearningmastery.com/bayes-theorem-for-machine-learning/\n",
        "\n",
        "**Marginal Probability:** The probability of an event irrespective of the outcomes of other random variables, e.g. P(A).\n",
        "\n",
        "**Conditional Probability:** Probability of one (or more) event given the occurrence of another event, e.g. P(A given B) or P(A | B).\n",
        "\n",
        "**Joint Probability:**  Probability of two (or more) simultaneous events, e.g. P(A and B) or P(A, B).\n",
        "\n",
        "The joint probability can be calculated using the conditional probability; for example:\n",
        "\n",
        "P(A, B) = P(A | B) * P(B)\n",
        "This is called the product rule. Importantly, the joint probability is symmetrical, meaning that:\n",
        "\n",
        "P(A, B) = P(B, A)\n",
        "\n",
        "The conditional probability can be calculated using the joint probability; for example:\n",
        "\n",
        "P(A | B) = P(A, B) / P(B)\n",
        "\n",
        "The conditional probability is not symmetrical; for example:\n",
        "\n",
        "P(A | B) != P(B | A)\n",
        "\n",
        "**An Alternate Way To Calculate Conditional Probability**\n",
        "Specifically, one conditional probability can be calculated using the other conditional probability; for example:\n",
        "\n",
        "P(A|B) = P(B|A) * P(A) / P(B)\n",
        "The reverse is also true; for example:\n",
        "\n",
        "P(B|A) = P(A|B) * P(B) / P(A)\n",
        "\n",
        "**Bayes Theorem:** Principled way of calculating a conditional probability without the joint probability.\n",
        "\n",
        "<img src=\"https://github.com/rnomadic/ML_Learning/blob/main/MyPythonCode/Bayes_Theorem.png?raw=1\" width=\"400\">\n",
        "\n",
        "It is often the case that we do not have access to the denominator directly, e.g. P(B).\n",
        "\n",
        "We can calculate it an alternative way; for example:\n",
        "\n",
        "P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)\n",
        "This gives a formulation of Bayes Theorem that we can use that uses the alternate calculation of P(B), described below:\n",
        "\n",
        "P(A|B) = P(B|A) * P(A) / P(B|A) * P(A) + P(B|not A) * P(not A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDFNiGR1I7s9"
      },
      "source": [
        "### Q2.7 How an effect size impacts results/decisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CrFZeymI7s-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PpvN0p7I7s-"
      },
      "source": [
        "### Q2.8 Mean, Variance for Normal, Uniform, Poisson"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVbnhrQYI7s-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izd5NNT7I7s-"
      },
      "source": [
        "### Q2.9 Sampling techniques and common designs (A/B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B42CLmsII7s-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85DzJfd8I7s-"
      },
      "source": [
        "### Q2.11 Common conjugate priors (Bayesian statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4BRazqaI7s-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmLmI9W-I7s-"
      },
      "source": [
        "### Q2.12 The various numerical optimization techniques (maximum likelihood, maximum a posteriori)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q 2.13 How SMOTE works\n",
        "Synthetic Minority Oversampling Technique (SMOTE).\n",
        "\n",
        "SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n",
        "\n",
        "The combination of SMOTE and under-sampling performs better than plain under-sampling.\n",
        "\n",
        "A general downside of the approach is that synthetic examples are created without considering the majority class, possibly resulting in ambiguous examples if there is a strong overlap for the classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "GhYDqFz2HTHL"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Probability_Statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}