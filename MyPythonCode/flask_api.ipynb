{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask framework\n",
    "If running locally - e.g. by starting the web service using python run flask_api.py - we would be able reach our function (or 'endpoint') at http://localhost:8080/predict. This function takes data sent to it as JSON (that has been automatically de-serialised as a Python dict made available as the request variable in our function definition), and returns a response (automatically serialised as JSON).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "#Write our predict method in predict.py\n",
    "import predict \n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def run():\n",
    "    data = request.get_json(force=True)\n",
    "    input_params = data['input']\n",
    "    result =  predict.predict(input_params)\n",
    "    return jsonify({'prediction': result})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why docker\n",
    "ML is a microservice and Docker is best to create that <br>\n",
    "A microservice is a software component that has the following properties:<br>\n",
    "    1. It does exactly one thing and does it well.\n",
    "    2. It is stateless.\n",
    "    3. Has a REST API for communication.\n",
    "If you easily want to create a microservice you should use Docker. It lets you containerize your application. This means that you can be sure that it runs exactly the same in every environment (there are some exceptions). It is like a little VM for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Defining the Docker Image with the Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.6-slim\n",
    "WORKDIR /usr/src/app\n",
    "COPY . .\n",
    "RUN pip install pipenv\n",
    "RUN pipenv install\n",
    "EXPOSE 8080\n",
    "CMD [\"pipenv\", \"run\", \"python\", \"flask_api.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Kubernetes\n",
    "Often in production, we have multiple microservices (Container) running together, that also have to talk to each other. This is where we would need a container orchestrator. Kubernetes is a great tool for doing this.\n",
    "\n",
    "We can run kubernetes on gcp using gcp sdk <br>\n",
    "\n",
    "gcloud components install kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our REST API is contained in the flask_api.py module, together with the Dockerfile, both within the py-flask-ml-score-api directory, whose core contents are as follows,\n",
    "\n",
    "py-flask-ml-score-api/\n",
    " | Dockerfile\n",
    " | Pipfile\n",
    " | Pipfile.lock\n",
    " | flask_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
